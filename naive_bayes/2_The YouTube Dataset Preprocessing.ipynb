{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebd4364",
   "metadata": {},
   "source": [
    "# Exercise - The YouTube Dataset: Preprocessing - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48026b9",
   "metadata": {},
   "source": [
    "### Introducing the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d52a7b",
   "metadata": {},
   "source": [
    "You can complete this exercise only after you have studied the following lessons:\n",
    "* **The YouTube Dataset: Classification**\n",
    "* **The YouTube Dataset: Confusion matrix** and\n",
    "* **The YouTube Dataset: Accuracy, Precision, Recall, and the F1 score**\n",
    "\n",
    "In this exercise, you will explore the consequences of transforming the dataset **before** train-test splitting by looking into the confusion matrix and classification report and comparing them with the results from the lessons. \n",
    "\n",
    "The code under the following sections is implemented: \n",
    "* **Importing the necessary libraries** - import all libraries necessary for completing the exercise.\n",
    "* **Reading the database** - concatenate the datasets from all 5 \".csv\" files into one dataframe and removing the unnecessary columns.\n",
    "* **Defining the inputs and the target** - define the inputs as all comments from the database. The targets are their respective classes.\n",
    "* **Tokenizing the YouTube comments** - create a vocabulary dictionary containing the words from all comments in the database.\n",
    "* **Creating the train-test split** - perform the train-tests split.\n",
    "\n",
    "Don't forget to put this notebook in the parent folder of the **youtube-dataset** folder. Otherwise, you would need to change the path under **Reading the database**.\n",
    "\n",
    "Please implement the relevant code from these lessons under the following sections:\n",
    "* **Performing the classification** - define a classifier and fit the model to the training data.\n",
    "* **Performing the evaluation on the test dataset** - make predictions on the test data. Using those predictions, construct a confusion matrix and a classification report.\n",
    "\n",
    "Please answer the following questions when you are ready:\n",
    "* What changes do you observe in the confusion matrix?\n",
    "* How do these changes affect the classification report?\n",
    "* How do you interpret the results?\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053831be",
   "metadata": {},
   "source": [
    "### Introducing the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b3cb7",
   "metadata": {},
   "source": [
    "The database for this example is taken from https://archive.ics.uci.edu/ml/machine-learning-databases/00380/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85d311",
   "metadata": {},
   "source": [
    "We usually modify the databases slightly such that they fit the purpose of the course. Therefore, we suggest you use the database provided in the resources in order to obtain the same results as the ones in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299f9e4",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ca2a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay,confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59c4c3",
   "metadata": {},
   "source": [
    "### Reading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f498b02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youtube-dataset\\\\Youtube01.csv',\n",
       " 'youtube-dataset\\\\Youtube02.csv',\n",
       " 'youtube-dataset\\\\Youtube03.csv',\n",
       " 'youtube-dataset\\\\Youtube04.csv',\n",
       " 'youtube-dataset\\\\Youtube05.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('youtube-dataset\\\\*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0d4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "\n",
    "for i in files:\n",
    "    all_df.append(pd.read_csv(i).drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3989fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey, check out my new website!! This site is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CONTENT  CLASS\n",
       "0     Huh, anyway check out this you[tube] channel: ...      1\n",
       "1     Hey guys check out my new channel and our firs...      1\n",
       "2                just for test I have to say murdev.com      1\n",
       "3               watch?v=vtaRGgvGtWQ   Check this out .﻿      1\n",
       "4     Hey, check out my new website!! This site is a...      1\n",
       "...                                                 ...    ...\n",
       "1950  I love this song because we sing it at Camp al...      0\n",
       "1951  I love this song for two reasons: 1.it is abou...      0\n",
       "1952                                                wow      0\n",
       "1953                            Shakira u are so wiredo      0\n",
       "1954                         Shakira is the best dancer      0\n",
       "\n",
       "[1955 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(all_df, axis=0, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167043b",
   "metadata": {},
   "source": [
    "### Defining the inputs and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['CONTENT']\n",
    "target = data['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8a85e",
   "metadata": {},
   "source": [
    "### Tokenizing the YouTube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8d139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297e9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_transf = vectorizer.fit_transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6cb5b",
   "metadata": {},
   "source": [
    "### Creating the train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfcd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs_transf, target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=365, \n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cdb00",
   "metadata": {},
   "source": [
    "### Performing the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce116cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb6e0b",
   "metadata": {},
   "source": [
    "### Performing the evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97ed6b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176  14]\n",
      " [  8 193]]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "sns.reset_orig()\n",
    "\n",
    "#ConfusionMatrixDisplay.from_predictions(\n",
    " #   y_test, y_test_pred,\n",
    "  #  labels = clf.classes_,\n",
    "   # cmap = 'magma'\n",
    "#);\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af5182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.96      0.93      0.94       190\n",
      "        Spam       0.93      0.96      0.95       201\n",
      "\n",
      "    accuracy                           0.94       391\n",
      "   macro avg       0.94      0.94      0.94       391\n",
      "weighted avg       0.94      0.94      0.94       391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred, target_names = ['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95ccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
